# 0.1.1.3 · Gap Analysis With Severity Scoring

- ## Objective
  - Produce an evidence-backed severity assessment for unmet TRD requirements so that leadership can prioritize remediation work.
- ## Deliverables
  - Python automation that ingests the existing traceability matrix and emits structured gap data.
  - JSON + Markdown reports summarizing severity, rationale, and mitigation guidance.
  - Execution log and roadmap updates documenting completion of roadmap item 0.1.1.3.
- ## Decision Tree
  - ### Source of Truth for Requirement Status
    - Option A: Re-scan repository artifacts and recompute requirement coverage from scratch.
    - Option B: Reuse the curated `traceability_matrix.json` produced in task 0.1.1.2.
    - **Chosen**: Option B to guarantee consistency with prior validation while avoiding redundant inventory sweeps.
  - ### Severity Scoring Strategy
    - Option A: Manual scoring embedded in Markdown.
    - Option B: Rule-based heuristic leveraging requirement metadata (IDs, keywords, descriptions) with extensible weights.
    - **Chosen**: Option B for repeatability and future automation.
  - ### Output Format
    - Option A: Markdown only (human readable).
    - Option B: JSON + Markdown pairing to feed future analytics pipelines.
    - **Chosen**: Option B following established reporting precedent.
- ## Implementation Phases
  - ### Phase 1 · Data Modeling
    - Design data classes/structures representing requirement gaps, severity details, and mitigation steps.
    - Define severity scale (critical/high/medium/low) mapped to numeric score ranges.
  - ### Phase 2 · Scoring Heuristics
    - Establish base scores per top-level requirement domain (ARCH, DATA, etc.).
    - Layer keyword/description boosts for services, environments, datasets, and security-critical phrases.
    - Adjust scores for partial coverage vs. completely missing implementations.
  - ### Phase 3 · Reporting Engine
    - Parse `traceability_matrix.json` and compute severity for each leaf requirement lacking full coverage.
    - Emit machine-readable JSON with generation metadata and aggregated summaries.
    - Render Markdown narrative highlighting critical gaps and recommended remediation priorities.
  - ### Phase 4 · Repository Integration
    - Store artifacts under `reports/due_diligence/` following existing naming conventions.
    - Update `ROADMAP.md` and `EXECUTION_LOG.md` to reflect task completion with detailed context.
  - ### Phase 5 · Verification
    - Run the gap analysis CLI to regenerate reports.
    - Review Markdown output for clarity and accuracy prior to commit.
- ## Notes to Future Self
  - Keep heuristic weights configurable to accommodate stakeholder feedback without rewriting code.
  - Ensure generated JSON enumerates rationale strings for auditability.
  - Consider integrating with future dashboards (Grafana/Metabase) once data foundations are in place.
